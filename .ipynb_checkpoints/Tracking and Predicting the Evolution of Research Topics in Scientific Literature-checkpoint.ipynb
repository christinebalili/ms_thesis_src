{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import networkx as nx \n",
    "import igraph as ig\n",
    "import numpy as np \n",
    "import scipy as sp \n",
    "import pickle\n",
    "import pandas as pd \n",
    "import collections\n",
    "import requests \n",
    "import math \n",
    "import json\n",
    "import re\n",
    "from imblearn.combine import SMOTETomek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructing the graphs from the co-occurrence logs in CSV format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_nx_graph(path,timestamp,graph_format=\"gpickle\",output_dir=\"graphs/\"):\n",
    "    graph_file = open(path,\"rb\")\n",
    "    co_occurrences = graph_file.readlines()\n",
    "    co_occurrences = [x.strip() for x in co_occurrences] \n",
    "    \n",
    "    graph = nx.parse_edgelist(co_occurrences, nodetype = str, data=(('weight',int),), delimiter=\",\")\n",
    "    \n",
    "    outfile = output_dir+\"graph_\"+str(timestamp)+\".\"+graph_format\n",
    "    if graph_format == \"gpickle\":\n",
    "        nx.write_gpickle(graph,outfile)\n",
    "    \n",
    "    elif graph_format == \"gexf\":\n",
    "        nx.write_gexf(graph,outfile)\n",
    "        \n",
    "    elif graph_format == \"gml\":\n",
    "        nx.write_gml(graph,outfile)\n",
    "        \n",
    "    elif graph_format == \"pajek\":\n",
    "        nx.write_pajek(graph,outfile)\n",
    "    \n",
    "    elif graph_format == \"edgelist\":\n",
    "        nx.write_weighted_edgelist(graph,outfile)\n",
    "    \n",
    "    return {\n",
    "        \"outputfile\":outfile,\n",
    "        \"graph\":graph\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Community detection at each snapshot using iGraph. There is no interface between networkx and iGraph; hence, from the raw_data, the networkx-constructed graph is saved into pajek format to be used by iGraph in community detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def detect_communities(graph,timestamp,method=\"infomap\",save_to_file=True,output_dir=\"graphs/\",file_format=\"pajek\"):\n",
    "    if method == \"infomap\":\n",
    "        results = graph.community_infomap(edge_weights=\"weight\")\n",
    "        \n",
    "    elif method == \"label_prop\":\n",
    "        results = graph.community_label_propagation(weights=\"weight\")\n",
    "        \n",
    "    elif method == \"multilevel\":\n",
    "        results = graph.community_multilevel(weights=\"weight\")\n",
    "        \n",
    "    elif method == \"walktrap\":\n",
    "        results = graph.community_walktrap(weights=\"weight\")\n",
    "    \n",
    "    elif method == \"louvain\":\n",
    "        results = graph.community_fastgreedy(weights=\"weight\")\n",
    "    \n",
    "    graph.vs[\"community\"] = results.membership\n",
    "    \n",
    "    if save_to_file: \n",
    "        graph.write_pajek(fname=output_dir + \"graph_\" + str(timestamp) + \"_\" + method + \"_comms.\"+file_format)\n",
    "    \n",
    "    return graph\n",
    "\n",
    "def get_assigned_community(node_id,graph):\n",
    "    assigned_comm = graph.vs[node_id][\"community\"]\n",
    "    return assigned_comm\n",
    "\n",
    "def get_community(community_id,graph):\n",
    "    nodes_in_comm = graph.vs.select(community_eq=community_id)\n",
    "    return [v.index for v in nodes_in_comm]\n",
    "\n",
    "def get_all_communities(graph,size=3):\n",
    "    communities = set(graph.vs[\"community\"])\n",
    "    all_communities = {}\n",
    "    for comm in communities:\n",
    "        nodes_in_comm = graph.vs.select(community_eq=comm)\n",
    "        if len(nodes_in_comm)>=size:\n",
    "            all_communities[comm] = [v.index for v in nodes_in_comm]\n",
    "    \n",
    "    return all_communities\n",
    "\n",
    "def get_vertex_names(nodes,graph):\n",
    "    return [graph.vs[node][\"id\"] for node in nodes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility functions for community matching and event detection across snapshots.\n",
    "Theta is the similarity threshold for matching a pair of communities. \n",
    "Phi is th fluctuation threshold that measures the change in size of a community that has a match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_similarity(comm1,comm2): #based on (Hopcroft et al.,2004)\n",
    "    common = set(comm1).intersection(set(comm2))\n",
    "    return min(len(common)*1.0/len(comm1), len(common)*1.0/len(comm2))\n",
    "\n",
    "def compute_fluctuation(comm1,comm2):\n",
    "    return (len(comm2)*1.0/len(comm1))-1\n",
    "\n",
    "def find_matches(graph1,graph2,theta=0.25,comm_size=3):\n",
    "    \n",
    "    communities1 = get_all_communities(graph1) #set of communities from snapshot 1\n",
    "    communities2 = get_all_communities(graph2) #set of communities from snapshot 2\n",
    "    \n",
    "    print \"There are %s communities/topics in snapshot i.\"%len(communities1)\n",
    "    print \"There are %s communities/topics in snapshot i+1.\"%len(communities2)\n",
    "    \n",
    "    matches = {} #contains the mapping of communities from snapshot 1 to snapshot 2\n",
    "    \n",
    "    for id1,nodes1 in communities1.iteritems():  #communities in time n\n",
    "        similarities = dict()\n",
    "        \n",
    "        if len(nodes1)>= comm_size:\n",
    "            nodes1 = get_vertex_names(nodes1,graph1)\n",
    "            \n",
    "            for id2,nodes2 in communities2.iteritems(): #communities in time n+1  \n",
    "                if len(nodes2) >= comm_size:\n",
    "                    nodes2 = get_vertex_names(nodes2,graph2)\n",
    "                    theta_p = compute_similarity(nodes1,nodes2)  \n",
    "                    similarities[id2]=theta_p\n",
    "            \n",
    "            non_zero = filter(lambda x: x != 0, similarities.values()) \n",
    "            match = [item for item in non_zero if item >= theta]\n",
    "\n",
    "            if len(match) >= 1:  #match found \n",
    "                matched = similarities.keys()[similarities.values().index(match[0])] \n",
    "                matches[len(matches)+1] = {\"1\": id1,\"2\":matched}\n",
    "\n",
    "            elif len(match) == 0: #match not found\n",
    "                matches[len(matches)+1] = {\"1\":id1,\"2\":-1} \n",
    "    \n",
    "    matches_df = pd.DataFrame.from_dict(matches,orient=\"index\")\n",
    "    return matches_df\n",
    "\n",
    "def detect_events(graph1,graph2,matches_df,base_timestamp,save_to_file=True,output_dir=\"model/\",phi=0.10):\n",
    "    \n",
    "    matches_df.columns = [\"snap1\",\"snap2\"]\n",
    "    \n",
    "    events = {} #events/labels of communities from snapshot 1 based on their match in snapshot 2 \n",
    "    #communities without a match are considered to have dissolved \n",
    "    dissolved = matches_df[matches_df[\"snap2\"]== -1]\n",
    "    \n",
    "    for community1 in dissolved[\"snap1\"]:\n",
    "        events[community1] = \"dissolve\"\n",
    "            \n",
    "    #detect merging \n",
    "    matches_df[\"merges\"] = matches_df.duplicated('snap2')\n",
    "    merged =  matches_df[(matches_df[\"merges\"] == True) & (matches_df[\"snap2\"] != -1)]\n",
    "    \n",
    "    for community1 in merged[\"snap1\"]:\n",
    "        events[community1] = \"merge\"\n",
    "    \n",
    "    #detect splitting \n",
    "    matches_df[\"splits\"] = matches_df.duplicated('snap1')\n",
    "    split = matches_df[(matches_df[\"splits\"] == True) & (matches_df[\"snap2\"] !=-1)]\n",
    "    \n",
    "    for community1 in merged[\"snap2\"]:\n",
    "        events[community1] = \"split\"\n",
    "    \n",
    "    #detect growth, shrink, and survival based on fluctuation threshold phi \n",
    "    matches_df_persist = matches_df[(matches_df[\"merges\"] == False) & (matches_df[\"snap2\"] !=-1) & (matches_df[\"splits\"]==False)]\n",
    "    for index, row in matches_df_persist.iterrows():\n",
    "        nodes1 = get_community(row[\"snap1\"],graph1)\n",
    "        nodes2 = get_community(row[\"snap2\"],graph2) \n",
    "        \n",
    "        fluctuation = compute_fluctuation(nodes1,nodes2)\n",
    "        \n",
    "        if fluctuation >= phi:\n",
    "            events[row[\"snap1\"]] = \"growth\"\n",
    "        \n",
    "        elif fluctuation <= -phi:\n",
    "            events[row[\"snap1\"]] = \"shrink\"\n",
    "            \n",
    "        else:\n",
    "            events[row[\"snap1\"]] = \"survive\"\n",
    "    \n",
    "    matches_df = matches_df[[\"snap1\",\"snap2\"]]\n",
    "    events_df = pd.DataFrame.from_dict(events,orient=\"index\")\n",
    "    mapping = matches_df.join(events_df,on=\"snap1\",how=\"inner\")\n",
    "    mapping.columns = [\"snap1\",\"snap2\",\"event\"]\n",
    "    \n",
    "    event_stats = mapping[[\"snap1\",\"event\"]].groupby(['event']).count()\n",
    "    print event_stats\n",
    "    \n",
    "    if save_to_file:\n",
    "        events_df.to_csv(output_dir+str(base_timestamp)+\"_events.csv\",index_label=\"id\")\n",
    "    \n",
    "    return {\n",
    "        \"events\" : events_df,\n",
    "        \"mapping\": mapping,\n",
    "        \"event_stats\": event_stats\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility functions for extracting features from communities in snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_node_features(graph,timestamp,save_to_file=True,file_format=\"pajek\",output_dir=\"graphs/\"):\n",
    "    graph.vs[\"pagerank\"] = graph.pagerank(directed=False,weights='weight')\n",
    "    \n",
    "    degree_centrality = graph.degree(graph.vs)\n",
    "    degree_centrality = (np.array(degree_centrality)*1.0)/np.sum(np.array(degree_centrality))\n",
    "    graph.vs[\"degree\"] = degree_centrality \n",
    "    \n",
    "    clustering_coefficient = graph.transitivity_local_undirected(weights='weight')\n",
    "    graph.vs[\"clustering_coefficient\"] = clustering_coefficient \n",
    "    \n",
    "    if save_to_file:\n",
    "        graph.write_pajek(output_dir + \"graph_\" + str(timestamp) + \"_node_atts.\" + file_format)\n",
    "        \n",
    "    return graph\n",
    "    \n",
    "def get_structural_features(communities,timestamp,graph,save_to_file=True,file_format=\"csv\",output_dir=\"model/\"):\n",
    "    feats = {}\n",
    "    \n",
    "    for community_id,nodes in communities.iteritems():\n",
    "        structural_feats = {}\n",
    "        community_subgraph = graph.subgraph([node for node in nodes])\n",
    "\n",
    "        inside_edges = graph.es.select(_within=nodes)\n",
    "        inside_weights = [e[\"weight\"] for e in inside_edges]\n",
    "\n",
    "        outside_edges = []\n",
    "        for node in nodes:\n",
    "            #incident_to_node = [e.tuple for e in graph.es.select(_source=node)]\n",
    "            incident_to_node = graph.es.select(_source=node)\n",
    "            incident_to_node = list(filter(lambda x: x not in inside_edges, incident_to_node))\n",
    "            outside_edges = outside_edges + incident_to_node\n",
    "\n",
    "        outside_weights = [e[\"weight\"] for e in outside_edges]\n",
    "\n",
    "        structural_feats[\"size\"] = len(nodes)\n",
    "        structural_feats['inside_edges'] = len(inside_edges)\n",
    "        structural_feats['inside_weights'] = np.sum(inside_weights)\n",
    "        structural_feats['outside_edges'] = len(outside_edges)\n",
    "        structural_feats['outside_weights'] = np.sum(outside_weights)\n",
    "        structural_feats['weight_ratio'] = structural_feats['inside_weights'] * 1.0/structural_feats['outside_weights']\n",
    "        total_comm_weight = structural_feats['inside_weights'] + structural_feats['outside_weights']\n",
    "        structural_feats['inside_weight_ratio'] = structural_feats['inside_weights'] * 1.0 / total_comm_weight\n",
    "        structural_feats['outside_weight_ratio'] = structural_feats['outside_weights'] * 1.0 /total_comm_weight \n",
    "\n",
    "        structural_feats['inside_weights_mean'] = np.mean(inside_weights)\n",
    "        structural_feats['inside_weights_75'] = np.percentile(inside_weights,75)\n",
    "        structural_feats['inside_weights_50'] = np.percentile(inside_weights,50)\n",
    "        structural_feats['inside_weights_25'] = np.percentile(inside_weights,25)\n",
    "        structural_feats['inside_weights_std'] = np.std(inside_weights)\n",
    "\n",
    "        if len(outside_edges) > 0:\n",
    "            cohesion = len(inside_edges)*1.0/len(outside_edges)\n",
    "            structural_feats['cohesion'] = cohesion\n",
    "            \n",
    "        possible_edges = len(nodes)*((len(nodes)-1))\n",
    "        non_edges = possible_edges - len(inside_edges)\n",
    "        non_edges = non_edges*1.0/possible_edges\n",
    "        structural_feats['unlinked'] = non_edges\n",
    "                                         \n",
    "        #node-specific properties\n",
    "        pageranks = community_subgraph.vs[\"pagerank\"]\n",
    "        degree_centrality = community_subgraph.vs[\"degree\"]  \n",
    "        clustering_coefficient = community_subgraph.vs[\"clustering_coefficient\"]\n",
    "            \n",
    "        pageranks = filter(lambda x: not np.isnan(x),pageranks)\n",
    "        degree_centrality = filter(lambda x: not np.isnan(x),degree_centrality)\n",
    "        clustering_coefficient = filter(lambda x: not np.isnan(x),degree_centrality)\n",
    "        \n",
    "        structural_feats['pageranks_mean'] = np.mean(pageranks)\n",
    "        structural_feats['pageranks_75'] = np.percentile(pageranks,75)\n",
    "        structural_feats['pageranks_50'] = np.percentile(pageranks,50)\n",
    "        structural_feats['pageranks_25'] = np.percentile(pageranks,25)\n",
    "        structural_feats['pageranks_std'] = np.std(pageranks)\n",
    "        structural_feats['pageranks_max'] = np.max(pageranks)\n",
    "                                         \n",
    "        structural_feats['degree_mean'] = np.mean(degree_centrality)\n",
    "        structural_feats['degree_75'] = np.percentile(degree_centrality,75)\n",
    "        structural_feats['degree_50'] = np.percentile(degree_centrality,50)\n",
    "        structural_feats['degree_25'] = np.percentile(degree_centrality,25)\n",
    "        structural_feats['degree_std'] = np.std(degree_centrality)\n",
    "        structural_feats['degree_max'] = np.max(degree_centrality)\n",
    "            \n",
    "        structural_feats['clustering_coeff_mean'] = np.mean(clustering_coefficient)\n",
    "        structural_feats['clustering_coefficient_75'] = np.percentile(clustering_coefficient,75)\n",
    "        structural_feats['clustering_coefficient_50'] = np.percentile(clustering_coefficient,50)\n",
    "        structural_feats['clustering_coefficient_25'] = np.percentile(clustering_coefficient,25)\n",
    "        structural_feats['clustering_coefficient_std'] = np.std(clustering_coefficient)\n",
    "        structural_feats['clustering_coefficient_max'] = np.max(clustering_coefficient)\n",
    "            \n",
    "        structural_feats['density'] = community_subgraph.density()\n",
    "        structural_feats['subgraph_clustering_coeff'] = community_subgraph.transitivity_avglocal_undirected()\n",
    "            \n",
    "        feats[community_id] = structural_feats\n",
    "            \n",
    "    #return dataframe of the structural features of communities\n",
    "    feats = pd.DataFrame.from_dict(feats,orient=\"index\")\n",
    "    feats = feats[~feats.isin([np.nan, np.inf, -np.inf]).any(1)]\n",
    "    feats = feats.dropna(axis=0,how='any')\n",
    "    \n",
    "    #normalize the data \n",
    "    feats_normed = feats\n",
    "    feats_normed = feats_normed.apply(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "    \n",
    "    if save_to_file:\n",
    "        #print feats\n",
    "        #print feats_normed\n",
    "        feats.to_csv(output_dir + str(timestamp) + \"_\" + \"structural_feats.\" + file_format,index_col=\"id\")\n",
    "        feats_normed.to_csv(output_dir + str(timestamp) + \"_\" + \"structural_feats_normed.\" + file_format,index_col=\"id\")\n",
    "        \n",
    "    return feats_normed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and testing the topic evolution prediction model. This is just a demo on a subset of the data originally used in the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph_dir = \"graphs/\" #where the constructed graphs will be stored. The graphs are weighted and undirected.\n",
    "data_dir = \"data/\"    #contains the pre-processed co-occurrence logs of format: keyword1,keyword2,frequency\n",
    "model_dir = \"model/\"  #topic evolution model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Two consecutive snapshots from the dynamic network\n",
    "timestamp1 = 1\n",
    "timestamp2 = 2\n",
    "\n",
    "#Build the graph based on the co-occurrences within the duration of the snapshot in timestamp1\n",
    "location1 = data_dir + \"cooc_1.csv\"\n",
    "output1 = make_graph(location1,timestamp1,graph_format=\"pajek\")\n",
    "graph1 = output1[\"graph\"]\n",
    "path1 = output1[\"outputfile\"]\n",
    "\n",
    "#Detect communities from the snapshot in timestamp1\n",
    "i_graph1 =  ig.Graph.Read_Pajek(path1)\n",
    "i_graph_comms1 = detect_communities(i_graph1,timestamp)\n",
    "\n",
    "#Build the graph based on the co-occurrences within the duration of the snapshot in timestamp2\n",
    "\n",
    "location2 = data_dir + \"cooc_2.csv\"\n",
    "output2 = make_graph(location2,timestamp2,graph_format=\"pajek\")\n",
    "graph2 = output2[\"graph\"]\n",
    "path2 = output2[\"outputfile\"]\n",
    "\n",
    "#Detect communities from the snapshot in timestamp2\n",
    "i_graph2 =  ig.Graph.Read_Pajek(path2)\n",
    "i_graph_comms2 = detect_communities(i_graph2,timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Get the set of communities discovered for a pair of network snapshots\n",
    "graph1 = ig.Graph.Read_Pickle(graph_dir+\"graph_1_comms.pickle\")\n",
    "graph2 = ig.Graph.Read_Pickle(graph_dir+\"graph_2_comms.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 644 communities/topics in snapshot i.\n",
      "There are 618 communities/topics in snapshot i+1.\n",
      "          snap1\n",
      "event          \n",
      "dissolve    154\n",
      "growth      160\n",
      "merge        34\n",
      "shrink      137\n",
      "split        13\n",
      "survive     146\n"
     ]
    }
   ],
   "source": [
    "#Matching communities across snapshots \n",
    "matches = find_matches(graph1,graph2)\n",
    "events = detect_events(graph1,graph2,matches,timestamp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Extract features for each topic found in the snapshot at timestamp1\n",
    "graph_with_new_atts = get_node_features(graph1,timestamp1)\n",
    "graph_communities = get_all_communities(graph_with_new_atts)\n",
    "features = get_structural_features(graph1_communities,timestamp1,graph_with_new_atts)\n",
    "feature_names = list(features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Prepare the dataset for training\n",
    "events[\"events\"].columns = [\"event\"]\n",
    "training_data = features.join(events[\"events\"],how=\"inner\")\n",
    "training_data = training_data.dropna(axis=0,how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset : 617\n",
      "Size of resampled dataset:  804\n"
     ]
    }
   ],
   "source": [
    "#Resampling the dataset \n",
    "X = training_data[feature_names]\n",
    "y = training_data[\"event\"]\n",
    "print \"Size of dataset :\",len(X)\n",
    "\n",
    "smote_tomek = SMOTETomek(random_state=1234)\n",
    "X_resampled, y_resampled = smote_tomek.fit_sample(X,y)\n",
    "\n",
    "print \"Size of resampled dataset: \",len(X_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Comparison of different models  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_cross_val(model,X,y):\n",
    "    accuracy = cross_val_score(model,X,y,cv=skf,scoring='accuracy')\n",
    "    precision = cross_val_score(model,X,y,cv=skf,scoring='precision_macro') \n",
    "    recall = cross_val_score(model,X,y,cv=skf,scoring='recall_macro') \n",
    "    f1 = cross_val_score(model,X,y,cv=skf,scoring='f1_macro') \n",
    "    \n",
    "    return {\n",
    "        'acc_mean': accuracy.mean(),\n",
    "        'acc_std': accuracy.std() * 2,\n",
    "        'prec_mean': precision.mean(), \n",
    "        'prec_std': precision.std() * 2, \n",
    "        'recall_mean': recall.mean(), \n",
    "        'recall_std': recall.std() * 2, \n",
    "        'f1_mean': f1.mean(), \n",
    "        'f1_std': f1.std() * 2\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             adaboost       knn   log_reg        rf       svm\n",
      "acc_mean     0.308407  0.493273  0.357670  0.584998  0.298789\n",
      "acc_std      0.096540  0.122992  0.108801  0.210882  0.056704\n",
      "f1_mean      0.279933  0.411611  0.279007  0.512190  0.193944\n",
      "f1_std       0.107162  0.143433  0.096148  0.194943  0.055675\n",
      "prec_mean    0.349305  0.418711  0.267748  0.537409  0.225855\n",
      "prec_std     0.198468  0.162770  0.073681  0.180314  0.106918\n",
      "recall_mean  0.295985  0.459194  0.334610  0.568496  0.265710\n",
      "recall_std   0.098579  0.124156  0.106901  0.182928  0.052581\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold, cross_val_score, cross_val_predict \n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pylab as plt\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "results = {}\n",
    "\n",
    "#Random Forest Classifier \n",
    "rfc = RandomForestClassifier(n_estimators=100)\n",
    "results[\"rf\"] = calculate_cross_val(rfc,X_resampled,y_resampled)\n",
    "\n",
    "#Support Vector Machine \n",
    "svc = SVC()\n",
    "results[\"svm\"] = calculate_cross_val(svc,X_resampled,y_resampled)\n",
    "\n",
    "#K-Nearest Neighbors \n",
    "knn = KNeighborsClassifier()\n",
    "results[\"knn\"] = calculate_cross_val(knn,X_resampled,y_resampled)\n",
    "\n",
    "#Logistic Regression\n",
    "log_reg = LogisticRegression()\n",
    "results[\"log_reg\"] = calculate_cross_val(log_reg,X_resampled,y_resampled)\n",
    "\n",
    "#Adaboost \n",
    "adaboost = AdaBoostClassifier()\n",
    "results[\"adaboost\"] = calculate_cross_val(adaboost,X_resampled,y_resampled)\n",
    "\n",
    "results = pd.DataFrame.from_dict(results)\n",
    "print results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Random Forest appears to outperform the other types of classifiers using its default number of trees = 10. \n",
    "We tune the number of estimators in this part. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               rfc_10   rfc_100    rfc_25    rfc_50    rfc_75\n",
      "acc_mean     0.545652  0.592117  0.570974  0.587387  0.582337\n",
      "acc_std      0.152354  0.193418  0.164159  0.172586  0.170033\n",
      "f1_mean      0.484736  0.524987  0.530235  0.527946  0.509277\n",
      "f1_std       0.187953  0.169748  0.157523  0.181822  0.187272\n",
      "prec_mean    0.517702  0.575099  0.511584  0.529847  0.564934\n",
      "prec_std     0.165409  0.203551  0.185800  0.184997  0.207904\n",
      "recall_mean  0.527280  0.571625  0.532573  0.564417  0.552989\n",
      "recall_std   0.178415  0.167604  0.185586  0.198131  0.180387\n"
     ]
    }
   ],
   "source": [
    "trees = [10,25,50,75,100]\n",
    "results = {}\n",
    "\n",
    "for n in trees:\n",
    "    rfc = RandomForestClassifier(n_estimators=n)\n",
    "    results[\"rfc_\"+str(n)] = calculate_cross_val(rfc,X_resampled,y_resampled)\n",
    "\n",
    "results = pd.DataFrame.from_dict(results)\n",
    "print results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the final working model to be used for predicting the evolution of the topics found in snapshot i+1 in the next snapshot i+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02518524 0.02333412 0.02904226 0.02769149 0.0339971  0.03101564\n",
      " 0.02488512 0.03136686 0.02284923 0.03250596 0.0271487  0.03048361\n",
      " 0.03161845 0.02418723 0.02352247 0.02795342 0.02055665 0.02991715\n",
      " 0.03161467 0.03377006 0.02752948 0.02804944 0.0267387  0.02577623\n",
      " 0.02927143 0.03035316 0.03033205 0.03201439 0.02536415 0.03891476\n",
      " 0.03620412 0.0311559  0.0312109  0.04443986]\n",
      "Importance of Features: \n",
      "[('inside_weights_std', 0.04443985615977736), ('pageranks_25', 0.03891476121394247), ('unlinked', 0.03620412327870571), ('inside_weights', 0.03399710483928934), ('inside_weights_50', 0.03377006270854713), ('pageranks_50', 0.032505963596189395), ('cohesion', 0.03201439421394688), ('pageranks_75', 0.03161844565656897), ('inside_weight_ratio', 0.031614668681590075), ('size', 0.031366856591139206), ('inside_weights_mean', 0.031210895014868716), ('outside_weights', 0.031155896069826973), ('inside_weights_75', 0.03101563901550026), ('pageranks_mean', 0.03048360525566303), ('density', 0.030353160753000166), ('pageranks_max', 0.030332050654768697), ('degree_75', 0.02991715423303519), ('degree_mean', 0.029271434290558597), ('inside_weights_25', 0.02904226386688379), ('clustering_coefficient_75', 0.028049444292635138), ('clustering_coeff_mean', 0.027953418212461206), ('outside_weight_ratio', 0.02769148525653299), ('degree_std', 0.027529480788077425), ('inside_edges', 0.02714869724942665), ('outside_edges', 0.02673870486424125), ('pageranks_std', 0.02577622535175295), ('degree_25', 0.025364146092947012), ('clustering_coefficient_std', 0.02518523671012802), ('clustering_coefficient_25', 0.024885123984635917), ('clustering_coefficient_50', 0.024187225059645524), ('subgraph_clustering_coeff', 0.02352246996749983), ('clustering_coefficient_max', 0.023334121184378224), ('degree_max', 0.022849233224887286), ('degree_50', 0.020556651666948512)]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "timestamp = int(time.time())\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=100)\n",
    "rfc.fit(X_resampled,y_resampled)\n",
    "\n",
    "feature_importance = rfc.feature_importances_\n",
    "print feature_importance \n",
    "\n",
    "rank = []\n",
    "for name, importance in zip(feature_names, feature_importance):\n",
    "    rank.append((name,importance))\n",
    "    \n",
    "rank = sorted(rank, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print \"Top 10 Features: \"\n",
    "print rank[:10]\n",
    "\n",
    "#Save model \n",
    "filename = 'rf_model_'+str(timestamp)+'.model'\n",
    "print \"Model filename :\" + filename\n",
    "pickle.dump(rfc, open(model_dir+filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Extract features for topic found in the snapshot at timestamp2\n",
    "graph_with_new_atts = get_node_features(graph2,timestamp2)\n",
    "graph_communities = get_all_communities(graph_with_new_atts)\n",
    "X_test = get_structural_features(graph_communities,timestamp2,graph_with_new_atts)\n",
    "X_test = X_test[feature_names] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Stats:\n",
      "prediction\n",
      "dissolve    144\n",
      "growth      113\n",
      "merge        35\n",
      "shrink      111\n",
      "split        42\n",
      "survive     150\n",
      "Name: prediction, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Using the resulting model to predict the evolution of topics at timestamp2\n",
    "filename = 'rf_model_1516164779.model'\n",
    "loaded_model = pickle.load(open(model_dir+filename, 'rb'))\n",
    "predictions = loaded_model.predict(X_test)\n",
    "X_test[\"prediction\"] = predictions\n",
    "\n",
    "print \"Prediction Stats:\"\n",
    "print X_test.groupby(['prediction'],axis=0)['prediction'].count() #outputs community_id, predicted topic evolution\n",
    "pd.DataFrame(X_test).to_csv(model_dir+str(timestamp2)+\"_predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring which topics will grow, shrink, survive?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Parameters for the PubMed API:\n",
    "user_num = \"129\"\n",
    "user_token = \"4n3-eeb91dbd494821a63a5c\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_label(cui):\n",
    "    request_url = \"http://havoc.appliedinformaticsinc.com/concepts/\"+cui+\"?sab=MSH&user=\"+user_num+\"&token=\"+user_token    \n",
    "    havoc_response = requests.get(request_url)\n",
    "        \n",
    "    try:\n",
    "        parents_res = havoc_response.json()\n",
    "        term = parents_res['terms'][0] \n",
    "        return term\n",
    "        \n",
    "    except (ValueError, IndexError):\n",
    "        return ' '\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_community_keywords(community_nodes,graph):\n",
    "    \n",
    "    community_subgraph = graph.subgraph([node for node in community_nodes])\n",
    "    pageranks = sorted(community_subgraph.vs[\"pagerank\"],reverse=True)\n",
    "    top_rank = pageranks[:10]\n",
    "    cuis = []\n",
    "    \n",
    "    for rank in top_rank:\n",
    "        cui = community_subgraph.vs.select(pagerank_eq=rank)[\"id\"][0]\n",
    "        cuis.append(cui)\n",
    "        \n",
    "    keywords = map(lambda x: get_label(x),cuis)\n",
    "    return list(keywords) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label of community index  0\n",
      "[u'Escherichia coli', u'Genes', u'DNA', u'Genes, Bacterial', u'Mutation', u'Transcription, Genetic', u'Gene Expression Regulation', u'Bacterial Proteins', u'RNA, Messenger', u'Saccharomyces cerevisiae']\n",
      "Label of community index  1\n",
      "[u'Occupational activity of managing finances', u'Hospital Departments', u'Personnel Management', u'Hospital Administration', u'Hospitals', u'Medicare', u'Nursing Staff, Hospital', u'Nursing Homes', u'Health Insurance', u'Discipline of Nursing']\n",
      "Label of community index  2\n",
      "[u'Brain', u'Neurons', u'Aging', u'Spinal Cord', u'Cerebral cortex', u'Norepinephrine', u'Dopamine', u'Serotonin', u'Retina', ' ']\n",
      "Label of community index  3\n",
      "[u'Monoclonal Antibodies', u'T-Lymphocyte', u'Lymphocyte', u'macrophage', u'neutrophil', u'B-Lymphocytes', u'Lymphocyte Activation', u'Immunoglobulin G', u'Surface Antigens', u'Autoantibodies']\n",
      "Label of community index  4\n",
      "[u'Hypertensive disease', u'Kidney', u'Coronary heart disease', u'Myocardium', u'Heart', u'Hemodynamics', u'Myocardial Infarction', u'Blood Pressure', u'Physical Exertion', u'Myocardial Contraction']\n"
     ]
    }
   ],
   "source": [
    "event = \"growth\" #replace with any event of interest \n",
    "\n",
    "#Sample topics that will grow\n",
    "growing_topics = X_test[X_test[\"prediction\"]==event].head()\n",
    "\n",
    "for index, row in growing_topics.iterrows():\n",
    "    community_nodes = get_community(index,graph2)\n",
    "    print \"Label of community index \",index\n",
    "    print get_community_keywords(community_nodes,graph2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
